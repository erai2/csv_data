import re
import json
import os
import glob
from typing import List, Dict, Any
import streamlit as st

from langchain.docstore.document import Document
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# --- ì„¤ì • ---
DB_DIRECTORY = "chroma_db"
JSON_FILE_PATH = "knowledge_base.json"
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

def parse_myeongri_document(text: str, source_file: str) -> List[Dict[str, Any]]:
    """ëª…ë¦¬í•™ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì²´ê³„ì ì¸ ì§€ì‹ chunkë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    structured_data = []
    delimiter_pattern = r'\n(?=Part\d+.*?|#\d+.*?|<ì‚¬ë¡€\s\d+>)'
    blocks = re.split(delimiter_pattern, text.strip())

    for block in blocks:
        block = block.strip()
        if not block: continue
        
        chunk = {"source_file": source_file}
        lines = block.split('\n')
        first_line = lines[0].strip()

        case_match = re.match(r'^<ì‚¬ë¡€\s(\d+)>', first_line)
        section_match = re.match(r'^(Part\d+.*?|#\d+.*?)', first_line)

        if case_match:
            chunk["category"] = "ì‚¬ë¡€"
            chunk["sub_category"] = f"ì‚¬ë¡€ {case_match.group(1)}"
            case_content = block
            
            title_match = re.search(r'^/ì œëª©/(.*)', case_content, re.MULTILINE)
            if title_match:
                chunk["title"] = title_match.group(1).strip()
                case_content = case_content.replace(title_match.group(0), '', 1)
            else:
                chunk["title"] = re.sub(r'<ì‚¬ë¡€\s\d+>\s*', '', first_line).strip() or chunk["sub_category"]
            
            structure_match = re.search(r'^êµ¬ì„±/(.*)', case_content, re.MULTILINE)
            if structure_match:
                chunk["structure"] = structure_match.group(1).strip()
                case_content = case_content.replace(structure_match.group(0), '', 1)
            else:
                chunk["structure"] = ""

            cleaned_text = re.sub(r'^<ì‚¬ë¡€\s\d+>.*?\n', '', case_content, count=1).strip()
            chunk["text"] = cleaned_text

        elif section_match:
            chunk["category"] = "ìš©ì–´/ê·œì¹™"
            chunk["sub_category"] = section_match.group(1).strip()
            chunk["title"] = chunk["sub_category"]
            chunk["text"] = block
        else:
            chunk["category"] = "ê¸°íƒ€"
            chunk["sub_category"] = "ì„œë¡ "
            chunk["title"] = lines[0] if len(lines[0]) < 50 else "ì†Œê°œ"
            chunk["text"] = block
            
        structured_data.append(chunk)
    return structured_data

def process_and_embed(input_dir: str, status_placeholder):
    """ì§€ì •ëœ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  .md, .txt íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ë²¡í„° DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    all_chunks = []
    files_to_process = glob.glob(os.path.join(input_dir, "*.md")) + glob.glob(os.path.join(input_dir, "*.txt"))

    if not files_to_process:
        status_placeholder.error(f"ì˜¤ë¥˜: '{input_dir}' ë””ë ‰í† ë¦¬ì—ì„œ ì²˜ë¦¬í•  íŒŒì¼(.md, .txt)ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return

    status_placeholder.info(f"ğŸ“ ì´ {len(files_to_process)}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
    for filepath in files_to_process:
        filename = os.path.basename(filepath)
        try:
            with open(filepath, 'r', encoding='utf-8') as f: content = f.read()
        except UnicodeDecodeError:
            with open(filepath, 'r', encoding='cp949') as f: content = f.read()
        except Exception as e:
            status_placeholder.error(f"ì˜¤ë¥˜: '{filename}' íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
            
        chunks = parse_myeongri_document(content, filename)
        all_chunks.extend(chunks)

    unique_chunks = []
    seen_signatures = set()
    for chunk in all_chunks:
        signature = (chunk.get('title', ''), chunk.get('text', ''))
        if signature not in seen_signatures:
            unique_chunks.append(chunk)
            seen_signatures.add(signature)
    
    status_placeholder.info(f"âœ¨ ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(all_chunks)}ê°œ chunk -> {len(unique_chunks)}ê°œ ê³ ìœ  chunk")
    
    for i, chunk in enumerate(unique_chunks):
        chunk['chunk_id'] = f"{i+1:03d}"

    # Save the parsed data to knowledge_base.json
    with open(JSON_FILE_PATH, "w", encoding="utf-8") as f:
        json.dump(unique_chunks, f, ensure_ascii=False, indent=2)
    
    documents = []
    for chunk in unique_chunks:
        page_content = f"ì œëª©: {chunk.get('title', '')}\nì¹´í…Œê³ ë¦¬: {chunk.get('sub_category', '')}\në‚´ìš©: {chunk.get('text', '')}"
        metadata = {k: v for k, v in chunk.items() if k != 'text'}
        doc = Document(page_content=page_content, metadata=metadata)
        documents.append(doc)
    
    if not documents:
        status_placeholder.warning("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ì–´ ë²¡í„° DB êµ¬ì¶•ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    status_placeholder.info(f"ğŸ§  ì„ë² ë”© ëª¨ë¸ '{EMBEDDING_MODEL_NAME}'ì„(ë¥¼) ë¡œë“œí•©ë‹ˆë‹¤...")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)

    status_placeholder.info(f"ğŸ’¾ '{DB_DIRECTORY}' ë””ë ‰í† ë¦¬ì— ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤...")
    Chroma.from_documents(
        documents=documents, 
        embedding=embeddings,
        persist_directory=DB_DIRECTORY
    )
    
    status_placeholder.success("âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

