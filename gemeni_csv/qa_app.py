import io
import os
import json
import re
import shutil
from pathlib import Path

import pandas as pd
import streamlit as st
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint
from langchain.chains import RetrievalQA
# ë¡œì»¬ ìœ í‹¸ë¦¬í‹° íŒŒì¼ ì„í¬íŠ¸
import data_utils

# --- ì„¤ì • ---
APP_ROOT = Path(__file__).resolve().parent
DATA_DIRECTORY = APP_ROOT / "data"
DB_DIRECTORY = data_utils.DB_DIRECTORY
JSON_FILE_PATH = data_utils.JSON_FILE_PATH
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
LLM_MODEL_REPO_ID = "google/gemma-2-9b-it"

# --- AI Q&A ë° LLM ê´€ë ¨ í•¨ìˆ˜ ---
@st.cache_resource
def load_vector_db():
    if not os.path.exists(DB_DIRECTORY): return None
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    return Chroma(persist_directory=DB_DIRECTORY, embedding_function=embeddings)

@st.cache_resource
def get_llm(hf_api_token):
    if not hf_api_token: return None
    try:
        return HuggingFaceEndpoint(
            repo_id=LLM_MODEL_REPO_ID, huggingfacehub_api_token=hf_api_token,
            temperature=0.1, max_new_tokens=2048
        )
    except Exception as e:
        st.error(f"LLM ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def create_qa_chain(vector_db, llm):
    if not llm: return None
    retriever = vector_db.as_retriever()
    return RetrievalQA.from_chain_type(
        llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True
    )

# --- Streamlit UI ---
st.set_page_config(page_title="AI ì‹œìŠ¤í…œ", layout="wide")
st.title("ğŸ”® AI í†µí•© ì‹œìŠ¤í…œ")

# --- ì‚¬ì´ë“œë°” ---
st.sidebar.header("API ì„¤ì •")
hf_token = st.sidebar.text_input("Hugging Face API í† í°", type="password")

st.sidebar.divider()
st.sidebar.header("ğŸ“‚ ë°ì´í„° ê´€ë¦¬")
uploaded_files = st.sidebar.file_uploader(
    "ì§€ì‹ íŒŒì¼(.md, .txt) ì—…ë¡œë“œ",
    type=["md", "txt"],
    accept_multiple_files=True
)
if uploaded_files:
    if not DATA_DIRECTORY.exists():
        DATA_DIRECTORY.mkdir(parents=True, exist_ok=True)
    file_names = []
    for uploaded_file in uploaded_files:
        file_path = DATA_DIRECTORY / uploaded_file.name
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        file_names.append(uploaded_file.name)
    st.sidebar.success(f"{len(file_names)}ê°œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {', '.join(file_names)}")

st.sidebar.divider()
st.sidebar.header("âš™ï¸ DB ê´€ë¦¬")
if st.sidebar.button("ë°ì´í„°ë² ì´ìŠ¤ ì¬ìƒì„±"):
    if not DATA_DIRECTORY.exists() or not any(DATA_DIRECTORY.iterdir()):
        st.sidebar.error(f"'{DATA_DIRECTORY}' í´ë”ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else:
        if os.path.exists(DB_DIRECTORY):
            shutil.rmtree(DB_DIRECTORY)
        status = st.sidebar.empty()
        with st.spinner("ë°ì´í„° ì²˜ë¦¬ ë° DB ìƒì„± ì¤‘..."):
            data_utils.process_and_embed(str(DATA_DIRECTORY), status)
        st.rerun()

# --- ë©”ì¸ í™”ë©´ íƒ­ ---
tab1, tab2, tab3 = st.tabs(["ğŸ¤– AI Q&A", "ğŸ“š ì§€ì‹ ë² ì´ìŠ¤ íƒìƒ‰ê¸°", "ğŸ—‚ï¸ Rule & ìš©ì–´ ì •ë¦¬"])

with tab1:
    st.header("AIì—ê²Œ ì§ˆë¬¸í•˜ê¸°")
    db = load_vector_db()
    if not db:
        st.info("ì‚¬ì´ë“œë°”ì—ì„œ ì§€ì‹ íŒŒì¼ì„ ì—…ë¡œë“œí•œ í›„ 'ë°ì´í„°ë² ì´ìŠ¤ ì¬ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ DBë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
    else:
        llm = get_llm(hf_token)
        qa_chain = create_qa_chain(db, llm)
        query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ì œì••ìˆ˜ë‹¨ì˜ ì¢…ë¥˜ì™€ ê°ê°ì˜ ì‘ìš©ì„ ì•Œë ¤ì£¼ì„¸ìš”.", key="qa_query")
        if query and qa_chain:
            with st.spinner("AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                result = qa_chain.invoke(query)
                st.subheader("AI ë‹µë³€")
                st.write(result["result"])
                with st.expander("ğŸ“š ë‹µë³€ ê·¼ê±° (AIê°€ ì°¸ì¡°í•œ ë¬¸ì„œ)"):
                    for doc in result["source_documents"]:
                        st.markdown(f"**- ì œëª©: {doc.metadata.get('title', 'N/A')}**")
                        st.caption(f"ë‚´ìš©: {doc.page_content}")
with tab2:
    st.header("ì§€ì‹ ë² ì´ìŠ¤ íƒìƒ‰ê¸° (knowledge_base.json)")
    try:
        with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:
            full_data = json.load(f)
        df = pd.DataFrame(full_data)
        st.dataframe(df, use_container_width=True)
    except FileNotFoundError:
        st.info(f"'{JSON_FILE_PATH}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'ë°ì´í„°ë² ì´ìŠ¤ ì¬ìƒì„±'ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

with tab3:
    st.header("Rule & ê°œë… ìš©ì–´ ìë™ ë¶„ë¥˜")
    if not os.path.exists(JSON_FILE_PATH):
        st.info(
            "knowledge_base.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ë°ì´í„° ì—…ë¡œë“œ í›„ 'ë°ì´í„°ë² ì´ìŠ¤ ì¬ìƒì„±'ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
        )
    else:
        rc_df = data_utils.load_rule_concept_dataframe()
        if rc_df.empty:
            st.info("ë¶„ë¥˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
        else:
            st.caption(
                f"ì´ {len(rc_df)}ê°œì˜ í•­ëª©ì„ ìë™ìœ¼ë¡œ 'rule'ê³¼ 'concept'ìœ¼ë¡œ ë¶„ë¥˜í–ˆìŠµë‹ˆë‹¤. í•„ìš”í•œ ê²½ìš° ì§ì ‘ ìˆ˜ì • í›„ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )

            distribution = rc_df["rc_category"].value_counts().to_dict()
            col_stats = st.columns(len(distribution) or 1)
            for (label, count), column in zip(distribution.items(), col_stats):
                with column:
                    st.metric(label=f"{label.upper()}", value=count)

            editable_df = rc_df.copy()
            editable_df["chunk_id"] = editable_df["chunk_id"].astype(str)
            editable_df["text_preview"] = editable_df["text"].apply(
                lambda text: (text[:180] + "...") if len(text) > 180 else text
            )

            display_columns = [
                "chunk_id",
                "source_file",
                "title",
                "rc_category",
                "key_phrases",
                "summary",
                "notes",
                "text_preview",
            ]

            editor = st.data_editor(
                editable_df[display_columns],
                hide_index=True,
                num_rows="dynamic",
                use_container_width=True,
                column_config={
                    "chunk_id": st.column_config.TextColumn("Chunk ID", disabled=True, width="small"),
                    "source_file": st.column_config.TextColumn("ì›ë³¸ íŒŒì¼", disabled=True, width="small"),
                    "title": st.column_config.TextColumn("ì œëª©", disabled=True, width="medium"),
                    "rc_category": st.column_config.SelectboxColumn(
                        "ë¶„ë¥˜", options=sorted(data_utils.RULE_CONCEPT_ALLOWED), help="rule/concept ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”."
                    ),
                    "key_phrases": st.column_config.TextColumn(
                        "í•µì‹¬ í‚¤ì›Œë“œ", help="ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.", width="medium"
                    ),
                    "summary": st.column_config.TextColumn(
                        "ìš”ì•½", help="í•µì‹¬ ë‚´ìš©ì„ ê°„ë‹¨íˆ ì •ë¦¬í•˜ì„¸ìš”.", width="large"
                    ),
                    "notes": st.column_config.TextColumn("ë¹„ê³ ", width="medium"),
                    "text_preview": st.column_config.TextColumn(
                        "ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°", disabled=True, width="large", help="ì›ë¬¸ ì¼ë¶€ë¥¼ ì°¸ê³ í•˜ì„¸ìš”."
                    ),
                },
                key="rule_concept_editor",
            )

            updated_df = rc_df.set_index("chunk_id")
            editor_no_preview = editor.drop(columns=["text_preview"]).set_index("chunk_id")
            for column in ["rc_category", "key_phrases", "summary", "notes"]:
                if column in editor_no_preview.columns:
                    updated_df[column] = editor_no_preview[column]
            updated_df = updated_df.reset_index()

            st.divider()
            save_col, download_col, upload_col = st.columns([1, 1, 1])

            with save_col:
                if st.button("ë³€ê²½ ì‚¬í•­ ì €ì¥", type="primary"):
                    try:
                        data_utils.persist_rule_concept_dataframe(updated_df)
                        st.success("CSVì™€ knowledge_base.jsonì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        st.experimental_rerun()
                    except Exception as exc:
                        st.error(f"ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}")

            with download_col:
                try:
                    validated = data_utils.validate_rule_concept_dataframe(updated_df)
                    csv_buffer = io.StringIO()
                    validated.to_csv(csv_buffer, index=False)
                    st.download_button(
                        label="CSV ë‹¤ìš´ë¡œë“œ",
                        data=csv_buffer.getvalue(),
                        file_name="rule_concept_dataset.csv",
                        mime="text/csv",
                    )
                except Exception as exc:
                    st.error(f"CSV ìƒì„± ì¤‘ ì˜¤ë¥˜: {exc}")

            with upload_col:
                uploaded_csv = st.file_uploader(
                    "CSV ì—…ë¡œë“œ", type="csv", key="rule_concept_csv_uploader"
                )
                if uploaded_csv is not None:
                    try:
                        uploaded_df = pd.read_csv(uploaded_csv)
                        data_utils.persist_rule_concept_dataframe(uploaded_df)
                        st.success("ì—…ë¡œë“œí•œ CSV ë‚´ìš©ì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.")
                        st.experimental_rerun()
                    except Exception as exc:
                        st.error(f"CSV ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {exc}")

