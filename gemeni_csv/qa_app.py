import streamlit as st
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint
from langchain.chains import RetrievalQA
import os
import json
import pandas as pd
import re
import shutil
# ë¡œì»¬ ìœ í‹¸ë¦¬í‹° íŒŒì¼ ì„í¬íŠ¸
import data_utils

# --- ì„¤ì • ---
DATA_DIRECTORY = "data"
DB_DIRECTORY = "chroma_db"
JSON_FILE_PATH = "knowledge_base.json"
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
LLM_MODEL_REPO_ID = "google/gemma-2-9b-it" 

# --- AI Q&A ë° LLM ê´€ë ¨ í•¨ìˆ˜ ---
@st.cache_resource
def load_vector_db():
    if not os.path.exists(DB_DIRECTORY): return None
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    return Chroma(persist_directory=DB_DIRECTORY, embedding_function=embeddings)

@st.cache_resource
def get_llm(hf_api_token):
    if not hf_api_token: return None
    try:
        return HuggingFaceEndpoint(
            repo_id=LLM_MODEL_REPO_ID, huggingfacehub_api_token=hf_api_token,
            temperature=0.1, max_new_tokens=2048
        )
    except Exception as e:
        st.error(f"LLM ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def create_qa_chain(vector_db, llm):
    if not llm: return None
    retriever = vector_db.as_retriever()
    return RetrievalQA.from_chain_type(
        llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True
    )

# --- Streamlit UI ---
st.set_page_config(page_title="AI ì‹œìŠ¤í…œ", layout="wide")
st.title("ğŸ”® AI í†µí•© ì‹œìŠ¤í…œ")

# --- ì‚¬ì´ë“œë°” ---
st.sidebar.header("API ì„¤ì •")
hf_token = st.sidebar.text_input("Hugging Face API í† í°", type="password")

st.sidebar.divider()
st.sidebar.header("ğŸ“‚ ë°ì´í„° ê´€ë¦¬")
uploaded_files = st.sidebar.file_uploader(
    "ì§€ì‹ íŒŒì¼(.md, .txt) ì—…ë¡œë“œ",
    type=["md", "txt"],
    accept_multiple_files=True
)
if uploaded_files:
    if not os.path.exists(DATA_DIRECTORY):
        os.makedirs(DATA_DIRECTORY)
    file_names = []
    for uploaded_file in uploaded_files:
        file_path = os.path.join(DATA_DIRECTORY, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        file_names.append(uploaded_file.name)
    st.sidebar.success(f"{len(file_names)}ê°œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {', '.join(file_names)}")

st.sidebar.divider()
st.sidebar.header("âš™ï¸ DB ê´€ë¦¬")
if st.sidebar.button("ë°ì´í„°ë² ì´ìŠ¤ ì¬ìƒì„±"):
    if not os.path.exists(DATA_DIRECTORY) or not os.listdir(DATA_DIRECTORY):
        st.sidebar.error(f"'{DATA_DIRECTORY}' í´ë”ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    else:
        if os.path.exists(DB_DIRECTORY):
            shutil.rmtree(DB_DIRECTORY)
        status = st.sidebar.empty()
        with st.spinner("ë°ì´í„° ì²˜ë¦¬ ë° DB ìƒì„± ì¤‘..."):
            data_utils.process_and_embed(DATA_DIRECTORY, status)
        st.rerun()

# --- ë©”ì¸ í™”ë©´ íƒ­ ---
tab1, tab2 = st.tabs(["ğŸ¤– AI Q&A", "ğŸ“š ì§€ì‹ ë² ì´ìŠ¤ íƒìƒ‰ê¸°"])

with tab1:
    st.header("AIì—ê²Œ ì§ˆë¬¸í•˜ê¸°")
    db = load_vector_db()
    if not db:
        st.info("ì‚¬ì´ë“œë°”ì—ì„œ ì§€ì‹ íŒŒì¼ì„ ì—…ë¡œë“œí•œ í›„ 'ë°ì´í„°ë² ì´ìŠ¤ ì¬ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ DBë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
    else:
        llm = get_llm(hf_token)
        qa_chain = create_qa_chain(db, llm)
        query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ì œì••ìˆ˜ë‹¨ì˜ ì¢…ë¥˜ì™€ ê°ê°ì˜ ì‘ìš©ì„ ì•Œë ¤ì£¼ì„¸ìš”.", key="qa_query")
        if query and qa_chain:
            with st.spinner("AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                result = qa_chain.invoke(query)
                st.subheader("AI ë‹µë³€")
                st.write(result["result"])
                with st.expander("ğŸ“š ë‹µë³€ ê·¼ê±° (AIê°€ ì°¸ì¡°í•œ ë¬¸ì„œ)"):
                    for doc in result["source_documents"]:
                        st.markdown(f"**- ì œëª©: {doc.metadata.get('title', 'N/A')}**")
                        st.caption(f"ë‚´ìš©: {doc.page_content}")
with tab2:
    st.header("ì§€ì‹ ë² ì´ìŠ¤ íƒìƒ‰ê¸° (knowledge_base.json)")
    try:
        with open(JSON_FILE_PATH, 'r', encoding='utf-8') as f:
            full_data = json.load(f)
        df = pd.DataFrame(full_data)
        st.dataframe(df, use_container_width=True)
    except FileNotFoundError:
        st.info(f"'{JSON_FILE_PATH}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'ë°ì´í„°ë² ì´ìŠ¤ ì¬ìƒì„±'ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")

